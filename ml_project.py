# -*- coding: utf-8 -*-
"""ML Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mpYEIWUNMMhGAzJAojT9s6VCU4sAWMhD
"""

pip install tensorflow

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns



from google.colab import files
uploaded = files.upload()


data = pd.read_csv('ai4i2020.csv')

# Display basic statistics for numeric columns
basic_stats = data.describe()
print("Basic Statistics:")
print(basic_stats)

# Check for missing values
missing_values = data.isnull().sum()
print("\nMissing Values:")
print(missing_values)

# Normalizing temperature and torque to comparable scales
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data[['Air temperature [K]', 'Process temperature [K]', 'Torque [Nm]']] = scaler.fit_transform(
    data[['Air temperature [K]', 'Process temperature [K]', 'Torque [Nm]']]
)


# Adding new features (Feature Engineering)
# Temperature differential
data['Temperature Differential'] = data['Process temperature [K]'] - data['Air temperature [K]']

# Torque to Speed Ratio
data['Torque to Speed Ratio'] = data['Torque [Nm]'] / data['Rotational speed [rpm]']

# Class imbalance handling is noted but not performed here, as it would be part of model training.

# 1 and 2. Distribution of Air and Process Temperature
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.histplot(data['Air temperature [K]'], kde=True, color='skyblue')
plt.title("Distribution of Air Temperature")
plt.xlabel("Normalized Air Temperature")

plt.subplot(1, 2, 2)
sns.histplot(data['Process temperature [K]'], kde=True, color='salmon')
plt.title("Distribution of Process Temperature")
plt.xlabel("Normalized Process Temperature")
plt.tight_layout()
plt.show()

# Explanation: These histograms show the distribution of air and process temperatures, helping us see if temperature ranges might contribute to failure patterns.

# 3. Boxplot of Torque by Product Quality Type
plt.figure(figsize=(10, 6))
sns.boxplot(x='Type', y='Torque [Nm]', data=data, palette="viridis")
plt.title("Torque Distribution by Product Quality Type")
plt.xlabel("Product Quality Type")
plt.ylabel("Normalized Torque [Nm]")
plt.show()

# Explanation: This boxplot illustrates torque differences across product types (L, M, H). Higher product quality may relate to torque usage differences, influencing wear and failure.


# Display the new data with additional engineered features
print("\nData with Engineered Features:")
print(data[['Air temperature [K]', 'Process temperature [K]', 'Torque [Nm]', 'Temperature Differential', 'Torque to Speed Ratio']].head())


# Filter only numeric columns for correlation
numeric_data = data.select_dtypes(include=['number'])

# Correlation Matrix
correlation_matrix = numeric_data.corr()

# Visualizing the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix of Features")
plt.show()


# Explanation: This heatmap helps identify strong correlations between variables,
# such as torque, rotational speed, and failure modes.


# Bar plot for failure mode occurrences
failure_modes = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']
failure_counts = data[failure_modes].sum()

plt.figure(figsize=(8, 5))
sns.barplot(x=failure_counts.index, y=failure_counts.values, palette="muted")
plt.title("Distribution of Failure Modes")
plt.xlabel("Failure Mode")
plt.ylabel("Count")
plt.show()

# Explanation: This bar plot shows the frequency of each failure mode,
# indicating which modes are more prevalent and require more focus.


# Scatter plot for failure analysis
plt.figure(figsize=(10, 6))
sns.scatterplot(data=data, x='Temperature Differential', y='Torque [Nm]', hue='Machine failure', palette="Set1", alpha=0.7)
plt.title("Temperature Differential vs Torque [Nm] with Machine Failure")
plt.xlabel("Temperature Differential")
plt.ylabel("Normalized Torque [Nm]")
plt.legend(title="Machine Failure")
plt.show()

# Explanation: This scatter plot shows how operational parameters like temperature
# differential and torque correlate with machine failure occurrences.

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import matplotlib.pyplot as plt
import seaborn as sns

# Define features (X) and target (y)
features = ['Air temperature [K]', 'Process temperature [K]', 'Torque [Nm]', 'Temperature Differential', 'Torque to Speed Ratio']
target = 'Machine failure'

X = data[features]
y = data[target]

# Address class imbalance using SMOTE
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Split the resampled data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Define the DNN model
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.4),
    Dense(64, activation='relu'),
    Dropout(0.4),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the DNN model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy', 'Recall'])

# Train the DNN model
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=30,
    batch_size=64
)

# Evaluate DNN model
test_loss, test_accuracy, test_recall = model.evaluate(X_test, y_test)
print(f"DNN Test Accuracy: {test_accuracy:.2f}, Recall: {test_recall:.2f}")

# Plot DNN training history
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('DNN Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Generate predictions for DNN
predictions = model.predict(X_test)
predicted_labels = (predictions > 0.5).astype(int).flatten()

# Evaluate DNN performance
print("DNN Classification Report:")
print(classification_report(y_test, predicted_labels))

cm = confusion_matrix(y_test, predicted_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Failure', 'Failure'],
            yticklabels=['No Failure', 'Failure'])
plt.title("DNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

roc_auc = roc_auc_score(y_test, predictions)
print(f"DNN ROC-AUC Score: {roc_auc:.2f}")

# Logistic Regression
lr = LogisticRegression(random_state=42, max_iter=500)
lr.fit(X_train, y_train)
lr_predictions = lr.predict(X_test)

print("Logistic Regression Accuracy:", accuracy_score(y_test, lr_predictions))
print("Logistic Regression Classification Report:")
print(classification_report(y_test, lr_predictions))

# Random Forest with hyperparameter tuning
rf = RandomForestClassifier(random_state=42, n_estimators=200, max_depth=20, min_samples_split=5)
rf.fit(X_train, y_train)
rf_predictions = rf.predict(X_test)

print("Random Forest Accuracy:", accuracy_score(y_test, rf_predictions))
print("Random Forest Classification Report:")
print(classification_report(y_test, rf_predictions))

# Ensemble Voting Classifier
voting_clf = VotingClassifier(
    estimators=[
        ('rf', rf),
        ('lr', lr)
    ],
    voting='soft'
)
voting_clf.fit(X_train, y_train)
voting_predictions = voting_clf.predict(X_test)

print("Voting Classifier Accuracy:", accuracy_score(y_test, voting_predictions))
print("Voting Classifier Classification Report:")
print(classification_report(y_test, voting_predictions))